# TALOS: A Modular and Automatic System for Open-Vocabulary Semantic Instance Segmentation

**TALOS** is a modular and extensible Computer Vision pipeline for performing **semantic instance segmentation** using an **open vocabulary** for semantic categories. Unlike conventional approaches (e.g., Detectron2) that are limited to a fixed set of object categories seen during training (such as those in the COCO dataset), TALOS identifies and segments object instances belonging to uncommon and diverse categories.

Many open-vocabulary detection and segmentation models require **user inputs** for semantic categories, which is impractical for automated applications like mobile robotics. TALOS addresses this limitation by automatically extracting semantic labels from images using **large-scale models** and then locating and segmenting the objects in the image.

The system is composed of three sequential stages: **Tagging**, **Location**, and **Segmentation**, each of which can be independently configured with state-of-the-art models.

[üìÑ Read the paper (Spanish PDF)](./docs/paper/talos_paper.pdf)

*This project is currently under development. Please check the **develop** branch for the latest updates.*

---

## Pipeline overview

TALOS takes an arbitrary number of **RGB images** as input and produces **instance-level segmentations** for each image, that include binary masks for each object instance, along with their corresponding bounding boxes and semantic labels.

The pipeline is designed to be **modular**, allowing for easy integration of new models and components. The three main stages of the pipeline are as follows:

### 1. Tagging
- **Description**: Extracts object category labels using large-scale models (LVLMs and/or LLMs).
- **Input**: RGB image.
- **Output**: List of semantic object categories (textual labels)
- **Tagging methods**:
  - **Direct Tagging**: Uses a Large Vision-Language Model (LVLM) to extract labels directly. A smaller and more specific model like RAM++ is suitable for this Tagging method too, but this is not as flexible as the LVLM approach.
  - **Tagging via LVLM Image Description and LLM Keyword Extraction**: Uses a LVLM to generate a description of the image, which is then processed by an LLM to extract keywords of the object categories that are present in the image description.

### 2. Location
- **Description**: Locates objects described by the category tags using a visual grounding model.
- **Inputs**:
  - RGB image
  - List of object labels (output from the Tagging stage).
- **Output**: List of bounding boxes with label and confidence.

### 3. Segmentation
- **Description**: Produces accurate instance segmentation masks using category-agnostic segmentation models.
- **Inputs**:
  - RGB image.
  - Located bounding boxes for each detected object (output from the Location stage).
- **Outputs**:
  - Binary masks, one per object instance.
  - Detections JSON that includes the semantic label, bounding box coordinates, location confidence score for each instance and mask ID.

---

## üß† Integrated technologies and models

### Tagging
- **Direct Tagging**: 
  - [Qwen](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct)
  - [Gemma 3](https://huggingface.co/google/gemma-3-27b-it)
  - [MiniCPM](https://huggingface.co/openbmb/MiniCPM-o-2_6)
  - [Recognize Anything Plus Model (RAM++)](https://github.com/xinyu1205/recognize-anything)

- **Tagging via LVLM Image Description and LLM Keyword Extraction**:
  - **LVLM Image Description**:
    - [Qwen](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct)
    - [LLaVA](https://ollama.com/library/llava)
  - **LLM Keyword Extraction**:
    - [DeepSeek](https://ollama.com/library/deepseek-r1)

### Location
- [Grounding DINO](https://huggingface.co/docs/transformers/model_doc/grounding-dino)

### Segmentation
- [Segment Anything Model 2 (SAM2)](https://github.com/facebookresearch/sam2)


---

## ‚öôÔ∏è Installation and Usage

### Installation

```bash
# Clone the repository
git clone https://github.com/macorisd/TALOS.git
cd TALOS
# Set up virtual environment
python3 -m venv venv
source venv/bin/activate
# Install dependencies
pip install -r requirements.txt
```

**_TODO_**: add instructions for installing the models

### Usage

**_TODO_**: more detailed usage instructions

For now, you can run the pipeline by running the pipeline_main.py script:
- You need to specify the input image name in the main function.
- You can change the selected models in the pipeline. Take a look at the following files:
  - pipeline/config/config.py
  - pipeline/config/config.json


---

## üìä Pipeline evaluation and results

**_TODO_**: add more detailed evaluation explanation

| TAGGING | LOCATION       | SEGMENTATION | Detection count | Tags Recall | Tags Accuracy | Bbox similarity | Mask similarity | FINAL SCORE |
|-|-|-|-|-|-|-|-|-|
| Qwen | Grounding DINO | SAM2 | 57.58 | 60.43 | 60.43 | 76.15 | 72.66 | 65.45



---

## ü§ù How to contribute

Contributions are welcome!

- Fork the repository.
- Create a new branch: git checkout -b feature/my-feature.
- Make your changes.
- Push to your fork: git push origin feature/my-feature.
- Submit a pull request with a clear description of your changes.

All pull requests will be reviewed and require approval before being merged into the main branch.


--- 

## üìö Citation

If you use TALOS in your research, please cite this repository as follows:

```bibtex
@misc{decena2025talos,
  author       = {Decena-Gimenez, Macoris},
  title        = {TALOS: A Modular and Automatic System for Open-Vocabulary Semantic Instance Segmentation},
  year         = {2025},
  howpublished = {\url{https://github.com/macorisd/TALOS}}
}
```

---


## üìÑ License

This project is licensed under the GPL-3.0 License. See the [LICENSE](./LICENSE) file for details.


---

## ‚úçÔ∏è Author's note

Hi! I'm Maco üëã‚Äã

This project is being developed as part of my Bachelor's Thesis in Software Engineering, as a member of the MAPIR research group at the University of M√°laga, Spain. It would not have been possible without the trust and support of my professors Javier and Ra√∫l. Thank you for believing in me!


---

## üì¨ Contact

If you have any questions, suggestions, or feedback, please reach out to me!

- Linkedin: [macorisd](https://www.linkedin.com/in/macorisd/)
- Email: [macorisd@gmail.com](mailto:macorisd@gmail.com)
